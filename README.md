<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="TensorFlow"/>
  <img src="https://img.shields.io/badge/Keras-Deep%20Learning-D00000?style=for-the-badge&logo=keras&logoColor=white" alt="Keras"/>
  <img src="https://img.shields.io/badge/Streamlit-UI-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" alt="Streamlit"/>
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License"/>
</p>

<h1 align="center">ğŸ–¼ï¸ Image Caption Generator</h1>

<p align="center">
  <em>An AI-powered deep learning application that generates natural language captions for images using a CNNâ€“LSTM encoder-decoder architecture, trained on the Flickr8k dataset.</em>
</p>

<p align="center">
  <a href="#-key-features">Features</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-demo">Demo</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-model-details">Model Details</a> â€¢
  <a href="#-project-structure">Project Structure</a> â€¢
  <a href="#-results">Results</a>
</p>

---

## âœ¨ Key Features

| Feature | Description |
|---|---|
| ğŸ§  **DenseNet201 Feature Extractor** | Leverages a pre-trained DenseNet201 CNN to extract rich 1920-dimensional image embeddings |
| ğŸ“ **LSTM Caption Decoder** | Generates fluent, human-readable captions word-by-word using sequence modelling |
| ğŸŒ **Streamlit Web Interface** | Clean, interactive UI â€” upload any image and get an AI-generated caption in seconds |
| âš¡ **End-to-End Pipeline** | From raw image â†’ feature extraction â†’ tokenized caption â€” fully automated inference |
| ğŸ“Š **Jupyter Notebook** | Complete, reproducible training pipeline with visualizations and learning curves |

---

## ğŸ—ï¸ Architecture

The model follows a classic **encoder-decoder** framework widely used in image captioning research:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IMAGE CAPTION GENERATOR                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   ğŸ“· Input Image (224Ã—224Ã—3)                                        â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚
â”‚   â”‚  DenseNet201  â”‚  â† Pre-trained on ImageNet                      â”‚
â”‚   â”‚  (Encoder)    â”‚                                                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚
â”‚          â”‚                                                           â”‚
â”‚          â–¼                                                           â”‚
â”‚   Image Features (1Ã—1920)                                           â”‚
â”‚          â”‚                                                           â”‚
â”‚          â–¼                                                           â”‚
â”‚   Dense(256, ReLU) â”€â”€â†’ Reshape(1, 256)                              â”‚
â”‚                              â”‚                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                    â”‚    Concatenate     â”‚                             â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                              â”‚                                       â”‚
â”‚   "startseq" â†’ Embedding(256) â†’ Text Embeddings                    â”‚
â”‚                              â”‚                                       â”‚
â”‚                              â–¼                                       â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                      â”‚  LSTM (256)   â”‚  â† Sequential word generation â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                             â”‚                                        â”‚
â”‚                      Dropout(0.5)                                    â”‚
â”‚                             â”‚                                        â”‚
â”‚                     Add (residual)                                   â”‚
â”‚                             â”‚                                        â”‚
â”‚                      Dense(128, ReLU)                                â”‚
â”‚                      Dropout(0.5)                                    â”‚
â”‚                             â”‚                                        â”‚
â”‚                      Dense(softmax)  â†’ Predicted Word                â”‚
â”‚                             â”‚                                        â”‚
â”‚                             â–¼                                        â”‚
â”‚                  "a dog playing in a field"                           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How it works:**
1. **Encoder** â€” DenseNet201 (pre-trained on ImageNet) extracts a 1920-dim feature vector from the input image.
2. **Decoder** â€” An LSTM network takes the image features concatenated with word embeddings and generates a caption one word at a time, starting from `startseq` until it predicts `endseq`.

---

## ğŸ¬ Demo

1. Launch the Streamlit app
2. Upload any `.jpg`, `.jpeg`, or `.png` image
3. The AI generates a descriptive caption and displays it overlaid on the image

```
ğŸ“·  Upload  â†’  ğŸ§  DenseNet201 + LSTM  â†’  ğŸ“ "two dogs are playing in the grass"
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python **3.8+**
- pip (Python package manager)

### 1. Clone the Repository

```bash
git clone https://github.com/prvn-kumar01/ImageCaptioning.git
cd ImageCaptioning
```

### 2. Install Dependencies

```bash
pip install tensorflow numpy matplotlib streamlit pillow
```

### 3. Run the Application

```bash
streamlit run main.py
```

The app will open in your browser at `http://localhost:8501`. Upload an image and see the magic! âœ¨

---

## ğŸ§ª Model Details

| Attribute | Value |
|---|---|
| **Feature Extractor** | DenseNet201 (pre-trained on ImageNet) |
| **Image Embedding Size** | 1920 dimensions |
| **Word Embedding Size** | 256 dimensions |
| **LSTM Hidden Units** | 256 |
| **Max Caption Length** | 34 tokens |
| **Input Image Size** | 224 Ã— 224 Ã— 3 |
| **Training Dataset** | [Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k) (~8,000 images, 5 captions each) |
| **Train/Val Split** | 85% / 15% |
| **Regularization** | Dropout (0.5) + Residual connection |
| **Framework** | TensorFlow / Keras |

### Training Pipeline

```
Flickr8k Dataset
      â”‚
      â”œâ”€â”€ Images â”€â”€â†’ DenseNet201 â”€â”€â†’ Feature Vectors (1920-d)
      â”‚
      â””â”€â”€ Captions â”€â”€â†’ Text Preprocessing â”€â”€â†’ Tokenization â”€â”€â†’ Padded Sequences
                                                                      â”‚
                                                                      â–¼
                                                           Custom Data Generator
                                                                      â”‚
                                                                      â–¼
                                                              CNN-LSTM Model
                                                                      â”‚
                                                                      â–¼
                                                          Trained Caption Model
```

**Text Preprocessing Steps:**
- Convert to lowercase
- Remove special characters and numbers
- Remove extra spaces and single characters
- Add `startseq` / `endseq` delimiters

---

## ğŸ“‚ Project Structure

```
ImageCaptioning/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                          # Streamlit app â€” upload image & generate caption
â”œâ”€â”€ ğŸ“„ README.md                        # You are here!
â”‚
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ ğŸ§  model.keras                  # Trained CNN-LSTM caption generation model (~52 MB)
â”‚   â”œâ”€â”€ ğŸ§  feature_extractor.keras      # DenseNet201 feature extraction model (~76 MB)
â”‚   â”œâ”€â”€ ğŸ“¦ tokenizer.pkl                # Fitted tokenizer with vocabulary mappings
â”‚   â””â”€â”€ ğŸ““ flickr8k-image-captioning-   # Full training notebook (Kaggle)
â”‚          using-cnns-lstms.ipynb
â”‚
â””â”€â”€ ğŸ“ input_image/                     # Directory for sample/test images
```

---

## ğŸ“ˆ Results

The model generates coherent, descriptive captions for a wide range of images. Some example outputs:

| Input | Generated Caption |
|---|---|
| ğŸï¸ Outdoor scene | *"a man is standing on a rock near the water"* |
| ğŸ• Dog photo | *"a brown dog is running through the grass"* |
| ğŸ‘¶ People | *"a child in a red shirt is playing with a ball"* |

> **Note:** The model was trained on 8,000 images. Performance can be significantly improved by training on larger datasets like Flickr30k or MS-COCO.

---

## ğŸ”® Future Improvements

- [ ] **Attention Mechanism** â€” Implement Bahdanau or Transformer-based attention for better spatial focus
- [ ] **Larger Datasets** â€” Train on Flickr30k / MS-COCO for richer vocabulary and accuracy
- [ ] **BLEU Score Evaluation** â€” Add automated caption quality metrics
- [ ] **Beam Search Decoding** â€” Replace greedy decoding with beam search for better captions
- [ ] **Docker Support** â€” Containerize the app for one-command deployment
- [ ] **REST API** â€” Add a FastAPI endpoint for programmatic access

---

## ğŸ§° Tech Stack

<p align="center">
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white" height="30"/>
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=flat-square&logo=tensorflow&logoColor=white" height="30"/>
  <img src="https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white" height="30"/>
  <img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&logo=numpy&logoColor=white" height="30"/>
  <img src="https://img.shields.io/badge/Matplotlib-11557C?style=flat-square&logo=matplotlib&logoColor=white" height="30"/>
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=flat-square&logo=streamlit&logoColor=white" height="30"/>
</p>

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to open an issue or submit a pull request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Praveen Kumar**

- GitHub: [@prvn-kumar01](https://github.com/prvn-kumar01)

---

<p align="center">
  <em>If you found this project useful, consider giving it a â­ on GitHub!</em>
</p>
